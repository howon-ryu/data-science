idx<- caret::createDataPartition(df$sex, p=0.7)
idx<- caret::createDataPartition(df$sex, p=0.7)
df_train<-df[idx$Resample1,]
df_test<-df[-idx$Resample1,]
df_train
model <- lm(sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data = df)
model
summary(model)
#
k <- ols_step_all_possible(model)
k
plot(k)
#
kk <- ols_step_all_possible(model)
kk
plot(kk)
#
kk <- ols_step_all_possible(model)
kk
plot(kk)
(
(
# 독립변수의 개수별로 가장 적합한 모델을 나타낸다
k <- ols_step_best_subset(model)
k
plot(k)
#stepwise forward regression by p value - (F-test)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k
plot(k)
#Stepwise Backward Regression by AIC
k <- ols_step_backward_aic(model) # for backward, ols_step_forward_aic()
k
plot(k)
head(df)
#stepwise forward regression by p value - (F-test)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
# 독립변수의 개수별로 가장 적합한 모델을 나타낸다
k <- ols_step_best_subset(model)
k
plot(k)
cvResults <- suppressWarnings(
CVlm(data = cars,
form.lm=dist ~ speed,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
attr(cvResults, 'ms')  # => 251.2783 mean squared error
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm= sex~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm= sex~ age,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm= sex~ age + hdlngth + skull,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm= sex~ age + hdlngth ,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
library(DAAG)
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm= sex~ age + hdlngth ,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
data <- read.csv('possum.csv')
# 데이터 분석
head(data)
summary(data)
# 1~3열 제거 (case: 순서, site : 덧에 잡힌 장소 번호 , pop: 인구..? 장소...?)
d<-data[,-c(1,2,3)]
head(d)
df <- na.omit(d)  # 모든 변수에 결측치 없는 데이터 추출
df
table(is.na(df))
summary(df)
head(df)
df$sex[df$sex == 'm'] <- 1
df$sex[df$sex == 'f'] <- 2
head(df)
#회귀 모델 만들기
ols_regress(sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data = df)
head(df)
model <- lm(sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data = df)
model
summary(model)
plot(model)
df_train<-df[idx$Resample1,]
df_test<-df[-idx$Resample1,]
df_train
lmMod <- lm(sex ~ , data=df_train)  # build the model
lmMod <- lm(sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data=df_train)  # build the model
distPred <- predict(lmMod, df_test)
distPred
summary (lmMod)
actuals_preds <- data.frame(cbind(actuals=df_test$sex, predicteds=distPred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)  # 82.7%
head(actuals_preds)
actuals_preds
str(data)
str(df)
type(df$sex)
dtype(df$sex)
summary(df)
actuals_preds <- data.frame(cbind(actuals=df_test$sex, predicteds=distPred))  # make actuals_predicteds dataframe.
actuals_preds
correlation_accuracy <- cor(actuals_preds)  # 82.7%
actuals_preds <- data.frame(cbind(actuals=as.numeric(df_test$sex), predicteds=distPred))  # make actuals_predicteds dataframe.
actuals_preds
correlation_accuracy <- cor(actuals_preds)  # 82.7%
head(actuals_preds)
correlation_accuracy <- cor(actuals_preds)  # 82.7%
correlation_accuracy
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
# => 58.42%, min_max accuracy
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
# 잔차
# intercept=> 기울기, 나머지는 각각의 y절편
# residuals: 잔차(직선을 구하기 위한 에러값)
ols_plot_resid_fit(model)
min_max accuracy
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
min_max_accuracy
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
mape
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm=sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm=as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=5,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
attr(cvResults, 'ms')
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm=as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=10,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
attr(cvResults, 'ms')
# 잔차
# intercept=> 기울기, 나머지는 각각의 y절편
# residuals: 잔차(직선을 구하기 위한 에러값)
ols_plot_resid_fit(model)
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm=as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=10,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
attr(cvResults, 'ms')
boxplot(df$sex, main="sex",
sub=paste("Outlier rows: ", boxplot.stats(df$age)$out))
boxplot(as.numeric(df$sex), main="sex",
sub=paste("Outlier rows: ", boxplot.stats(df$age)$out))
# box plot for 'speed'
boxplot(cars$dist, main="Distance",
sub=paste("Outlier rows: ", boxplot.stats(cars$dist)$out))
boxplot(as.numeric(df$sex), main="sex",
sub=paste("Outlier rows: ", boxplot.stats(df$age+df$hdlngth)$out))
library(ggplot2)
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) +
+   geom_density(mapping=aes(x=sex, colour = ))
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) + geom_density(mapping=aes(x=sex, colour = ))
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) + geom_density(mapping=aes(x=sex,))
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) + geom_density(mapping=aes(x=sex))
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
rmse_test_lm<-rmse(distPred,df_test$sex)
library(mlbench)
install.packages("mlbench")
library(mlbench)
rmse_test_lm<-rmse(distPred,df_test$sex)
rmse<-function(y1,y2){
sqrt(mean((y1-y2)^2))
}
rmse_test_lm<-rmse(distPred,df_test$sex)
distPred
df_test$sex
rmse_test_lm<-rmse(distPred,as.numeric(df_test$sex))
rmse_test_lm
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm=as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=10,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
attr(cvResults, 'ms')
cvResults
attr(cvResults, 'rms')
attr(cvResults, 'rmse')
attr(cvResults, 'ms')
cv_rms<-sqrt(cv_ms)
cv_ms<-attr(cvResults, 'ms')
cv_rms<-sqrt(cv_ms)
cv_rms
rmse_test_lm
# 잔차
# intercept=> 기울기, 나머지는 각각의 y절편
# residuals: 잔차(직선을 구하기 위한 에러값)
ols_plot_resid_fit(model)
ols_plot_resid_fit_spread(model)
#
kk <- ols_step_all_possible(model)
kk
# 독립변수의 개수별로 가장 적합한 모델을 나타낸다
k <- ols_step_best_subset(model)
k
#stepwise forward regression by p value - (F-test)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
#Stepwise Backward Regression by AIC
# aic는 낮을 수록 좋은것! 따라서 aic가 높은 예측변수들을 하나씩 빼나가는 과정!
k <- ols_step_backward_aic(model) # for backward, ols_step_forward_aic()
k
#Stepwise Regression by p vlaue
k <- ols_step_both_p(model) # for aic, ols_step_both_p()
k
plot(k)
```{r setup, include=FALSE}
```{r cars}
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
# 라이브러리 포함
library(olsrr)
library(dplyr)
library(DAAG)
library(ggplot2)
library(mlbench)
# 데이터 포함 및 데이터 확인인
data <- read.csv('possum.csv')
# 데이터 포함 및 데이터 확인인
data <- read.csv('possum.csv')
summary(data)
head(data)
d<-data[,-c(1,2,3)]
head(d)
df <- na.omit(d)  # 모든 변수에 결측치 없는 데이터 추출
df
table(is.na(df))
summary(df)
head(df)
df$sex[df$sex == 'm'] <- as.numeric(1)
df$sex[df$sex == 'f'] <- as.numeric(2)
head(df)
str(df)
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
# 예측모델 트레인 테스트 나누기
# library(caret)
ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p1
```{r }
```{r}
p1<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p1
p1<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=hdlngth, colour = sex))
p3<-ggplot(data=df) + geom_density(mapping=aes(x=skullw, colour = sex))
p4<-ggplot(data=df) + geom_density(mapping=aes(x=totlngth, colour = sex))
p5<-ggplot(data=df) + geom_density(mapping=aes(x=taill, colour = sex))
p6<-ggplot(data=df) + geom_density(mapping=aes(x=footlgth, colour = sex))
p7<-ggplot(data=df) + geom_density(mapping=aes(x=earconch, colour = sex))
p8<-ggplot(data=df) + geom_density(mapping=aes(x=eye, colour = sex))
p9<-ggplot(data=df) + geom_density(mapping=aes(x=chest, colour = sex))
p10<-ggplot(data=df) + geom_density(mapping=aes(x=belly, colour = sex))
p1+p2+p3
p1+p2+p3
# 라이브러리 포함
library(olsrr)
library(dplyr)
library(DAAG)
library(ggplot2)
library(mlbench)
library('tidyverse')
# 라이브러리 포함
library(olsrr)
library(dplyr)
library(DAAG)
library(ggplot2)
library(mlbench)
library(tidyverse)
p1+p2+p3
# 라이브러리 포함
library(olsrr)
library(dplyr)
library(DAAG)
library(ggplot2)
library(mlbench)
library(tidyverse)
install.packages('tidyverse')
install.packages("tidyverse")
# 라이브러리 포함
library(olsrr)
library(dplyr)
library(DAAG)
library(ggplot2)
library(mlbench)
library(tidyverse)
p1+p2+p3
p1<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=hdlngth, colour = sex))
p3<-ggplot(data=df) + geom_density(mapping=aes(x=skullw, colour = sex))
p4<-ggplot(data=df) + geom_density(mapping=aes(x=totlngth, colour = sex))
p5<-ggplot(data=df) + geom_density(mapping=aes(x=taill, colour = sex))
p6<-ggplot(data=df) + geom_density(mapping=aes(x=footlgth, colour = sex))
p7<-ggplot(data=df) + geom_density(mapping=aes(x=earconch, colour = sex))
p8<-ggplot(data=df) + geom_density(mapping=aes(x=eye, colour = sex))
p9<-ggplot(data=df) + geom_density(mapping=aes(x=chest, colour = sex))
p10<-ggplot(data=df) + geom_density(mapping=aes(x=belly, colour = sex))
p3
p2
p1+p2
# 예측모델 트레인 테스트 나누기
# library(caret)
p1<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=hdlngth, colour = sex))
p1
p2
library(tidyverse)
p1+p2
library('tidyverse')
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
# 예측모델 트레인 테스트 나누기
# library(caret)
p1<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=hdlngth, colour = sex))
p1
p2
p1+p2
library(dplyr)
library(patchwork)
p1+p2
# 라이브러리 포함
library(olsrr)
library(dplyr)
library(DAAG)
library(ggplot2)
library(mlbench)
library(tidyverse)
library(patchwork)
p1<-ggplot(data=df) + geom_density(mapping=aes(x=age, colour = sex))
p2<-ggplot(data=df) + geom_density(mapping=aes(x=hdlngth, colour = sex))
p3<-ggplot(data=df) + geom_density(mapping=aes(x=skullw, colour = sex))
p4<-ggplot(data=df) + geom_density(mapping=aes(x=totlngth, colour = sex))
p5<-ggplot(data=df) + geom_density(mapping=aes(x=taill, colour = sex))
p6<-ggplot(data=df) + geom_density(mapping=aes(x=footlgth, colour = sex))
p7<-ggplot(data=df) + geom_density(mapping=aes(x=earconch, colour = sex))
p8<-ggplot(data=df) + geom_density(mapping=aes(x=eye, colour = sex))
p9<-ggplot(data=df) + geom_density(mapping=aes(x=chest, colour = sex))
p10<-ggplot(data=df) + geom_density(mapping=aes(x=belly, colour = sex))
p1+p2
p1+p2+p3/p4+p5+p6
(p1+p2+p3)/p4+p5+p6
(p1+p2+p3)/(p4+p5+p6)
(p1+p2+p3)/(p4+p5+p6)/(p7+p8+p9+p10)
set.seed(1000)
idx<- caret::createDataPartition(df$sex, p=0.7)
df_train<-df[idx$Resample1,]
df_test<-df[-idx$Resample1,]
df_train
lmMod <- lm(sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data=df_train)  # build the model
distPred <- predict(lmMod, df_test)
distPred
summary (lmMod)
actuals_preds <- data.frame(cbind(actuals=as.numeric(df_test$sex), predicteds=distPred))  # make actuals_predicteds dataframe.
actuals_preds
correlation_accuracy <- cor(actuals_preds)
correlation_accuracy
head(actuals_preds)
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
min_max_accuracy #0.74 => 실제값과 예측값이 얼마나 근접한가
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
mape # 절대 백분율 편차 0.35 => 오차와 실제의 비율
rmse<-function(y1,y2){
sqrt(mean((y1-y2)^2))
}
rmse_test_lm<-rmse(distPred,as.numeric(df_test$sex))
rmse_test_lm
cvResults <- suppressWarnings(
CVlm(data = df,
form.lm=as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly,
m=10,
dots=FALSE,
seed=29,
legend.pos="topleft",
printit=FALSE,
main="Small symbols are predicted values while bigger ones are actuals."));
# performs the CV
cv_ms<-attr(cvResults, 'ms')
cv_rms<-sqrt(cv_ms)
cv_rms
model <- lm(sex ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data = df)
model
summary(model)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
plot(k)
model <- lm(as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data = df)
model
summary(model)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
plot(k)
#stepwise forward regression by p value - (F-test)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
plot(k)
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
plot(k)
var.test(model)
#Stepwise Backward Regression by AIC
# aic는 낮을 수록 좋은것! 따라서 aic가 높은 예측변수들을 하나씩 빼나가는 과정!
k <- ols_step_backward_aic(model) # for backward, ols_step_forward_aic()
model <- lm(as.numeric(sex) ~ age + hdlngth + skullw + totlngth+taill+footlgth+earconch+eye+chest+belly, data = df)
model
summary(model)
ols_plot_resid_fit(model)
ols_plot_resid_fit(model)
ols_plot_resid_fit_spread(model)
k <- ols_step_best_subset(model)
k <- ols_step_best_subset(model)
k
k <- ols_step_forward_p(model) # for backward,  ols_step_backward_p()
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
plot(k)
k <- ols_step_best_subset(model)
k
k <- ols_step_forward_p(model) #
k #eye>totlngth>hdlngth>chest 전방 선택시 순서로 중요한 변수임
plot(k)
k <- ols_step_backward_aic(model) # for backward, ols_step_forward_aic()
k
plot(k)
k <- ols_step_both_p(model) # for aic, ols_step_both_p()
k
plot(k)
# 독립변수의 개수별로 가장 적합한 모델을 나타낸다
k <- ols_step_best_subset(model)
k
k <- ols_step_best_subset(model)
k
