library(Amelia)
library(tidyverse)
library(ggplot2)
library(caret)
library(psych)
library(Amelia)
library(mice)
library(GGally)
library(rpart)
#library(randomForest)
library(klaR) # for NaiveBayes (=nb) function
library(naivebayes)
library(mice)
library(e1071)
library(dplyr)
library(caret)
library(rattle)
library(rpart)
data <- iris
data
data[, -5]
set.seed(100)
missmap(data)
ggplot(data, aes(Sepal.Length, colour = Sepal.Length)) +
geom_freqpoly(binwidth = 1) + labs(title="Sepal.Length Distribution by Outcome")
ggplot(data, aes(Sepal.Width, colour = Sepal.Width)) +
geom_freqpoly(binwidth = 1) + labs(title="Sepal.Width Distribution by Outcome")
ggplot(data, aes(Petal.Length, colour = Petal.Length)) +
geom_freqpoly(binwidth = 1) + labs(title="Petal.Length Distribution by Outcome")
ggplot(data, aes(Petal.Width, colour = Petal.Width)) +
geom_freqpoly(binwidth = 1) + labs(title="Petal.Width Distribution by Outcome")
# 트레이닝/ 테스트 셋 구성
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,] # 트레이닝
#training
testing <- data[-indxTrain,]
#testing# 테스트
#Species 비율
prop.table(table(training$Species)) * 100 #트레이닝의 Species 비율
prop.table(table(testing$Species)) * 100 #테스트의 Species 비율
#트레이닝 셋 변수 나누기
x = training[,-5] #예측변수
#x
y = training$Species # 목표 변수
#y
# rpart모델 만들기
fit <- rpart(Species~.,
data = training,
minsplit = 2,
minbucket = 1,
maxdepth = 30,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
tmp <- data.frame(testing, pred)
acc <- sum(tmp$Species == tmp$pred) / nrow(tmp)
acc # 0.9166
#나이브 베이지안 모델 생성
model <- naiveBayes(x,training$Species)
#model
#프레딕트
p <- predict(model, testing, type='class')
#p
#분류모델 평가(예측 모델 평가)
t<- table(p, testing$Species)
#t
#분류 정확도
(t[1,1]+t[2,2]+t[3,3])/nrow(testing)
#confusionMatrix(p, testing$Species) # 0.9444
#-------------------------------------------
#일반모델 vs naive => 0.9166vs0.9444
#-------------------------------------------
fit <- rpart(Species~.,
data = training,
minsplit = 2,
minbucket = 1,
maxdepth = 30,
cp = -1, method = "class")
fit
pdf("tree.pdf")
fancyRpartPlot(fit)
dev.off()
# 트레이닝/ 테스트 셋 구성
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,] # 트레이닝
#training
testing <- data[-indxTrain,]
#testing# 테스트
#Species 비율
prop.table(table(training$Species)) * 100 #트레이닝의 Species 비율
prop.table(table(testing$Species)) * 100 #테스트의 Species 비율
#트레이닝 셋 변수 나누기
x = training[,-5] #예측변수
#x
y = training$Species # 목표 변수
#y
# rpart모델 만들기
fit <- rpart(Species~.,
data = training,
minsplit = 2,
minbucket = 1,
maxdepth = 30,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
tmp <- data.frame(testing, pred)
acc <- sum(tmp$Species == tmp$pred) / nrow(tmp)
acc # 0.9166
#나이브 베이지안 모델 생성
model <- naiveBayes(x,training$Species)
#model
#프레딕트
p <- predict(model, testing, type='class')
#p
#분류모델 평가(예측 모델 평가)
t<- table(p, testing$Species)
#t
#분류 정확도
(t[1,1]+t[2,2]+t[3,3])/nrow(testing)
#confusionMatrix(p, testing$Species) # 0.9444
#-------------------------------------------
#일반모델 vs naive => 0.9166vs0.9444
#-------------------------------------------
fit <- rpart(Species~.,
data = training,
minsplit = 2,
minbucket = 1,
maxdepth = 30,
cp = -1, method = "class")
fit
pdf("tree.pdf")
fancyRpartPlot(fit)
dev.off()
#sepal.length로 sepal.width예측 acc = 0.16666
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,c(1,2)] # 트레이닝
#training
testing <- data[-indxTrain,c(1,2)]
#testing# 테스트
fit <- rpart(Sepal.Width~.,
data = training,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
#testing
tmp <- data.frame(testing, pred)
#tmp
#tmp$pred
acc <- sum(tmp$Sepal.Width == tmp$pred) / nrow(tmp)
acc
#sepal.length로 petal.length예측 acc = 0.0833
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,c(1,3)] # 트레이닝
#training
testing <- data[-indxTrain,c(1,3)]
#testing# 테스트
fit <- rpart(Petal.Length~.,
data = training,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
#testing
tmp <- data.frame(testing, pred)
#tmp
#tmp$pred
acc <- sum(tmp$Petal.Length == tmp$pred) / nrow(tmp)
acc
#sepal.length로 petal.width예측 acc = 0.2222
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,c(1,4)] # 트레이닝
#training
testing <- data[-indxTrain,c(1,4)]
#testing# 테스트
fit <- rpart(Petal.Width~.,
data = training,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
#testing
tmp <- data.frame(testing, pred)
#tmp
#tmp$pred
acc <- sum(tmp$Petal.Width == tmp$pred) / nrow(tmp)
acc
#sepal.width로 petal.length예측 acc = 0.08333
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,c(2,3)] # 트레이닝
#training
testing <- data[-indxTrain,c(2,3)]
#testing# 테스트
fit <- rpart(Petal.Length~.,
data = training,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
#testing
tmp <- data.frame(testing, pred)
#tmp
#tmp$pred
acc <- sum(tmp$Petal.Length == tmp$pred) / nrow(tmp)
acc
#petal.length로 petal.width예측 acc = 0.25
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,c(3,4)] # 트레이닝
#training
testing <- data[-indxTrain,c(3,4)]
#testing# 테스트
fit <- rpart(Petal.Width~.,
data = training,
cp = -1, method = "class")
#fit
pred = predict(fit, testing, type = "class" )
#pred
#testing
tmp <- data.frame(testing, pred)
#tmp
#tmp$pred
acc <- sum(tmp$Petal.Width == tmp$pred) / nrow(tmp)
acc
indxTrain <- createDataPartition(y = data$Species,p = 0.75,list = FALSE)
#indxTrain
training <- data[indxTrain,] # 트레이닝
#training
testing <- data[-indxTrain,]
#testing# 테스트
#training
x = training[,-4] #예측변수
x
#sepal.length, petal.length 독립성이 큰 예측 변수 쌍 + pdf
# 2.6<petal.lengh<4.8 sepal.length>=5 =>31%
x[,c(1,3,4)]
fit_ind_high <- rpart(Species~.,
data = x[,c(1,3,4)],
minsplit = 2,
minbucket = 1,
maxdepth = 30,
cp = -1, method = "class")
fit_ind_high
pdf("tree_ind_high.pdf")
fancyRpartPlot(fit_ind_high)
dev.off()
# 분자 - p(x1,x2|y)
ind_x1_x2_y<-0
ind_y<-0
x[i,c(1,3)]
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_y = ind_y+1
if(x[i,3]>2.6){
if(x[i,3]<4.8){
if(x[i,1]>=5){
ind_x1_x2_y= ind_x1_x2_y+1
}
}
}
}
}
ind_y
ind_x1_x2_y
ind_p_x1_x2_y<-ind_x1_x2_y/ind_y
ind_p_x1_x2_y
# 분자-p(y)
ind_result<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_result = ind_result+1
}
}
ind_p_y<-result/nrow(x)
ind_p_y
# 총 분자
ind_bayes_up<-ind_p_x1_x2_y*ind_p_y
ind_bayes_up
# 분모-p(x1,x2)
ind_x1_x2<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,3]>2.6){
if(x[i,3]<4.8){
if(x[i,1]>=5){
ind_x1_x2= ind_x1_x2+1
}
}
}
}
ind_bayes_down<-ind_x1_x2/nrow(x)
ind_bayes_down
# 독립성 큰 베이지안 확률=>1
ind_bayes<-ind_bayes_up/ind_bayes_down
ind_bayes
# 분자 p(y)
ind_result<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_result = ind_result+1
}
}
ind_p_y<-result/nrow(x)
ind_p_y
# 분자 p(x1|y)
ind_nresult<-0
ind_n_p_x1<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_nresult = ind_nresult+1
if(x[i,3]>2.6){
if(x[i,3]<4.8){
ind_n_p_x1 = ind_n_p_x1+1
}
}
}
}
ind_n_p_x1_y<-ind_n_p_x1/ind_nresult
ind_n_p_x1_y
#분자 p(x2|y)
ind_nresult<-0
ind_n_p_x2<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_nresult = ind_nresult+1
if(x[i,1]>=5){
ind_n_p_x2=ind_n_p_x2+1
}
}
}
ind_n_p_x2_y<-ind_n_p_x2/ind_nresult
ind_n_p_x2_y
# 독립성 큰 나이브 베이지안 총 분자
ind_nbayes_up<-ind_n_p_x1_y*ind_n_p_x2_y*ind_p_y
ind_nbayes_up
# 독립성 큰 나이브 베이지안 분모
ind_n_x1_x2<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,3]>2.6){
if(x[i,3]<4.8){
if(x[i,1]>=5){
ind_n_x1_x2= ind_n_x1_x2+1
}
}
}
}
ind_nbayes_down<-ind_n_x1_x2/nrow(x)
ind_nbayes_down
# 독립성 큰 나이브 베이지안 확률 =>1
ind_nbayes<-ind_nbayes_up/ind_nbayes_down
ind_nbayes
#sepal.length, sepal.width 독립성이 작은 예측 변수 쌍
#  width>2.7 , legth<5.35 ==> 27% (setosa)
cor(data[,-5])
x[,c(1,2,4)]
fit_ind_low <- rpart(Species~.,
data = x[,c(1,2,4)],
minsplit = 2,
minbucket = 1,
maxdepth = 30,
cp = -1, method = "class")
fit_ind_low
pdf("tree_ind_low.pdf")
fancyRpartPlot(fit_ind_low)
dev.off()
#분자 = py
result<-0
for(i in 1:114){
if(x[i,4]=="setosa"){
result = result+1
}
}
p_y<-result/nrow(x)
p_y
# 분자 - p(x1,x2|y)
x1_x2_y<-0
y<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="setosa"){
y = y+1
if(x[i,1]<5.35){
if(x[i,2]>2.7){
x1_x2_y= x1_x2_y+1
}
}
}
}
#y
p_x1_x2_y<-x1_x2_y/y
p_x1_x2_y
# 총 분자
bayes_up<-p_y*p_x1_x2_y
bayes_up
#분모 p(x1,x2)
x1_x2<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,1]<5.35){
if(x[i,2]>2.7){
x1_x2 = x1_x2+1
}
}
}
bayes_down<-x1_x2/nrow(x)
bayes_down
# 베이즈 확률 = >1
bayes<-bayes_up/bayes_down
bayes
# 분자 - py
nrow(x)
x
summary(x)
result<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="setosa"){
result = result+1
}
}
p_y<-result/nrow(x)
p_y
# 분자 - p(x1,y)
x1<-0
y<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="setosa"){
y = y+1
if(x[i,1]<5.35){
x1 = x1+1
}
}
}
x1
p_x1_y<-x1/y
p_x1_y
# 분자 - p(x2,y)
x2<-0
y<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="setosa"){
y = y+1
if(x[i,2]>2.7){
x2 = x2+1
}
}
}
x2
p_x2_y<-x2/y
p_x2_y
#전체 분자
np_up<-p_y*p_x1_y*p_x2_y
np_up
# 분모
x1_x2<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,1]<5.35){
if(x[i,2]>2.7){
x1_x2 = x1_x2+1
}
}
}
np_down<-x1_x2/nrow(x)
np_down
#나이브 베이지안 확률=> 1.005093
np_result<-np_up/np_down
np_result
#나이브 베이지안 확률=> 1.005093
np_result<-np_up/np_down
np_result
# 분자 - p(x1,x2|y)
ind_x1_x2_y<-0
ind_y<-0
x[i,c(1,3)]
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_y = ind_y+1
if(x[i,3]>2.6){
if(x[i,3]<4.8){
if(x[i,1]>=5){
ind_x1_x2_y= ind_x1_x2_y+1
}
}
}
}
}
ind_y
ind_x1_x2_y
ind_p_x1_x2_y<-ind_x1_x2_y/ind_y
ind_p_x1_x2_y
# 분자-p(y)
ind_result<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_result = ind_result+1
}
}
ind_p_y<-ind_result/nrow(x)
ind_p_y
# 분자 p(y)
ind_result<-0
for(i in 1:114){ #최대 깊이를 1~20
if(x[i,4]=="versicolor"){
ind_result = ind_result+1
}
}
ind_p_y<-ind_result/nrow(x)
ind_p_y
